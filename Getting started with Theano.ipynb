{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Getting started with Theano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab was created by Jon Barker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following timer counts down to a five minute warning before the lab instance shuts down.  You should get a pop up at the five minute warning reminding you to save your work!  If you are about to run out of time, please see the [Post-Lab](#Post-Lab-Summary) section for saving this lab to view offline later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe id=\"timer\" src=\"timer/timer.html\" width=\"100%\" height=\"120px\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Before we begin, let's verify [WebSockets](http://en.wikipedia.org/wiki/WebSocket) are working on your system.  To do this, execute the cell block below by giving it focus (clicking on it with your mouse), and hitting Ctrl-Enter, or pressing the play button in the toolbar above.  If all goes well, you should see some output returned below the grey cell.  If not, please consult the [Self-paced Lab Troubleshooting FAQ](https://developer.nvidia.com/self-paced-labs-faq#Troubleshooting) to debug the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"The answer should be three: \" + str(1+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's execute the cell below to display information about the GPUs running on the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Theano is a Python library that allows you to write symbolic mathematical expressions and compile them onto different computational architectures (in particular, CPU and GPU). It was developed by machine learning researchers at the University of Montreal. Its use is not limited to machine learning applications, but it was designed with machine learning in mind. It's especially good for machine learning techniques which are CPU-intensive and benefit from parallelization (e.g. large neural networks).\n",
    "\n",
    "In this class we will demonstrate Theano's explicit representation of symbolic maths and it's ability to compile that representation into efficient code to be applied to numerical data. We are going to work through one of the most fundamental applications of articial neural networks - the multi-layer perceptron.  This example will allow us to see a complete neural network being defined and trained from scratch, but the process we will follow in Theano is essentially the same for some of the much more complicated networks, such as Convolutional Neural Networks (CNNs), that we have seen in our previous classes.  We will finish the class by getting you started in extending the multi-layer perceptron example into a CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Theano concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by introducing some important general concepts that form the basis for all computation in Theano.\n",
    "\n",
    "First we must import Theano into Python.  Notice that Theano is using the GPU - there is no more GPU configuration required when using Theano, the GPU will be leveraged for all applicable computations.  This was achieved by creating a file `.theanorc` file in our home directory that specified the flag `device=gpu`.  This configuration file is referenced by Theano when it is imported into Python.\n",
    "\n",
    "Execute the cell below to import Theano into Python.\n",
    "\n",
    "You will know the lab is processing when you see a solid circle in the top-right of the window that looks like this: ![](jupyter_executing.png)\n",
    "Otherwise, when it is idle, you will see the following: ![](jupyter_idle.png)\n",
    "If you ever feel like a cell has run for to long, you can stop it with the stop button in the toolbar.\n",
    "For troubleshooting, please see [Self-paced Lab Troubleshooting FAQ](https://developer.nvidia.com/self-paced-labs-faq#Troubleshooting) to debug the issue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "# By convention, the tensor submodule is loaded as T\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symbolic variables\n",
    "\n",
    "In Theano algorithms are defined symbolically.  This means that variables don't have to have explicit values.  The `theano.tensor` sub-module has various symbolic variable types.  Below is an example of a simple symbolic calculation using scalar variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a scalar variable\n",
    "a = T.scalar('a')\n",
    "\n",
    "# Define another variable as a squared\n",
    "b = a**2\n",
    "\n",
    "# b will also be a theano variable\n",
    "print type(b)\n",
    "\n",
    "# We can inspect how b is defined using theano's pretty print function\n",
    "print theano.pp(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n",
    "To compute things in Theano using actual numerical values you define symbolic functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can't compute anything with a, b and c yet.\n",
    "# We need to define a theano function first.\n",
    "# The first argument of theano.function defines the inputs\n",
    "f = theano.function([a],b)\n",
    "print f(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In some cases you can use a symbolic variable's eval method.\n",
    "# This can be more concise and convenient that defining a function.\n",
    "# If it has inputs, the eval method takes a Python dictionary where\n",
    "# the keys are theano variables and the values are values for those\n",
    "# variables.\n",
    "print b.eval({a: 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### theano.tensor\n",
    "\n",
    "Theano has variable types for vectors, matrices and tensors.  The `theano.tensor` sub-module has various functions for performing operations on these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define symbolic matrix and vector variables\n",
    "A = T.matrix('A')\n",
    "x = T.vector('x')\n",
    "\n",
    "# Define a symbolic function using these variables\n",
    "y = T.dot(A, x)\n",
    "\n",
    "# Define a theano function to compute y\n",
    "linear_func = theano.function([A, x], y)\n",
    "\n",
    "# Supply numeric values for A and x\n",
    "print linear_func(np.array([[1,2,3],[4,5,6]], dtype=theano.config.floatX),\n",
    "                  np.array([1,2,3], dtype=theano.config.floatX))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`theano.config.floatX` is a configuration variable that Theano also obtains from the `.theanorc` file.  Here it is used to specify the numerical type of the variables A and x.  If you are using GPU acceleration then `floatX` should be `float32`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared variables\n",
    "\n",
    "Shared variables are Theano variables that do have an explcit value that can be get/set and is shared across all functions which use the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shared_var = theano.shared(np.array([[1,2],[3,4]], dtype=theano.config.floatX))\n",
    "print shared_var.type()\n",
    "print shared_var.get_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shared_var.set_value(np.array([[3, 4], [2, 1]], dtype=theano.config.floatX))\n",
    "print shared_var.get_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shared_squared = shared_var**2\n",
    "# The first argument of theano.function (inputs) tells Theano what the arguments to the compiled function should be.\n",
    "# Note that because shared_var is shared, it already has a value, so it doesn't need to be an input to the function.\n",
    "# Therefore, Theano implicitly considers shared_var an input to a function using shared_squared and so we don't need\n",
    "# to include it in the inputs argument of theano.function.\n",
    "function_1 = theano.function([], shared_squared)\n",
    "print function_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### updates\n",
    "\n",
    "The value of a shared variable can be updated in a function by using the `updates` argument of `theano.function`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can also update the state of a shared var in a function\n",
    "subtract = T.matrix('subtract')\n",
    "# updates takes a dict where keys are shared variables and values are the new value the shared variable should take\n",
    "# Here, updates will set shared_var = shared_var - subtract\n",
    "function_2 = theano.function([subtract], shared_var, updates={shared_var: shared_var - subtract})\n",
    "print \"shared_var before subtracting [[1, 1], [1, 1]] using function_2:\"\n",
    "print shared_var.get_value()\n",
    "# Subtract [[1, 1], [1, 1]] from shared_var\n",
    "function_2(np.array([[1, 1], [1, 1]], dtype=theano.config.floatX))\n",
    "print \"shared_var after calling function_2:\"\n",
    "print shared_var.get_value()\n",
    "# Note that this also changes the output of function_1, because shared_var is shared!\n",
    "print \"New output of function_1() (shared_var**2):\"\n",
    "print function_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen the basic building blocks used in Theano we going to see a more practical application.  We're going to generate some toy data, simply two classes of points embedded in 2D space and seperated by the line $y=x$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import some necessary Python visualization modules\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "# Generate 50 random 2-D points\n",
    "x = np.random.random((50,2)).astype('float32')\n",
    "\n",
    "# If point is above y=x then class is 1,\n",
    "# otherwise class is 0\n",
    "y = np.zeros((50,)).astype('int32')\n",
    "y[x[:,0]>x[:,1]] = 1\n",
    "\n",
    "# Plot the points \n",
    "plt.scatter(x[:,0], x[:,1], c=y, s=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a probabilistic linear classifier.  That means it uses a linear model to predict the probability of the class that a data sample belongs to, where a data sample is a real-valued input vector.  Logistic regression achieves this by projecting these input vectors onto a set of hyperplanes, each one representing a class, and the distance of the data sample from the hyperplane boundary represents the probability that it is a member of that class.\n",
    "\n",
    "Logistic regression is parameterized by a weight matrix $W$ and a bias vector $b$.  The probability that an input vector $x$ belongs to class $i$ can be written as: \n",
    "\n",
    "$$\\Pr(Y=i \\mid x,W,b)=softmax_i(Wx+b)$$\n",
    "\n",
    "The linear function $Wx+b$ is equivalent to an artificial neural network layer where all input neurons are fully-connected to all output neurons.  The number of input neurons is the same as the number of dimensions in the input vector.  The dimensions of $W$ determine how many neurons are in the outut layer and this is equal to the number of classes.  The $softmax$ function simply \"squashes\" an arbitrary real-valued vector into a vector of the same dimensions but with values in the range (0,1) - in this example, softmax is applied to the output of $Wx+b$ so that we get probabilities of class membership.\n",
    "\n",
    "Using Theano to express the mathematics, we define a logistic regression model as a Python class with a method that initializes the parameters as random real-valued numbers in [0,1] and feeds-forward an input vector.  We also define a method that will allow us to use the model in it's current state to classify new data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "    \n",
    "    def __init__(self, input, n_in, n_out):\n",
    "\n",
    "        # Initialize pseudo-random number generator\n",
    "        rng = np.random.RandomState(1234)\n",
    "        \n",
    "        # Randomly generate weight matrix W\n",
    "        W_values = np.asarray(\n",
    "            rng.uniform(\n",
    "                low=-np.sqrt(6. / (n_in + n_out)),\n",
    "                high=np.sqrt(6. / (n_in + n_out)),\n",
    "                size=(n_in, n_out)\n",
    "            ),\n",
    "            dtype=theano.config.floatX\n",
    "        )\n",
    "\n",
    "        self.W = theano.shared(value=W_values, name='W', borrow=True)        \n",
    "\n",
    "        # Randomly initialize bias vector b\n",
    "        b_values = np.zeros((n_out,), dtype=theano.config.floatX)\n",
    "        self.b = theano.shared(value=b_values, name='b', borrow=True)\n",
    "\n",
    "        # Define feed-forward function\n",
    "        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)\n",
    "        \n",
    "        # Specify parameters of layer\n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "    def predict(self, input):\n",
    "        \n",
    "        # Function to apply layer feedforward to new input data\n",
    "        return T.nnet.softmax(T.dot(input, self.W) + self.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we define the two parameters of the logistic regression as Theano shared variables.  This declares them both as being symbolic Theano variables that mantain a persistent state throughout training the model.  Shared variables must have an explicit value set - they are not purely symbolic.\n",
    "\n",
    "We can test our new class by passing in a random input vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define theano shared variables for the input data and labels\n",
    "shared_x = theano.shared(np.asarray(x, dtype=theano.config.floatX), borrow=True)\n",
    "shared_y = theano.shared(np.asarray(y, dtype=theano.config.floatX), borrow=True)\n",
    "shared_y = T.cast(shared_y, 'int32')\n",
    "\n",
    "# Initialize a logistic regressor with 2-dimensional inputs\n",
    "# and 2 classes for output with the first row of x as input\n",
    "lr = LogisticRegression(shared_x[0,:], 2, 2)\n",
    "# Feed the first data sample from x through the regressor and print output\n",
    "print lr.p_y_given_x.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the feed-forward step of the logistic regression is just multiplication by a 2-dimensional matrix, we can actually pass into our class a whole matrix of input vectors where each row represents an individual data sample.  This is a good example of why GPU acceleration can be of great benefit in neural network training - these types of matrix multiplication commonly occur and can be parallelized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize a logistic regressor with 2-dimensional inputs\n",
    "# and 2 classes for output but now with all of x as input\n",
    "lr = LogisticRegression(shared_x, 2, 2)\n",
    "# Print the first 10 data samples from x through the regressor and print output\n",
    "print lr.p_y_given_x.eval()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So far our model is not very useful as all of the parameters have random values. In order to train the parameters of the network we must define a loss function.  A loss function measures how close the models predicted classification of a training data sample is to the actual known classification.  The training process will attempt to find values of $W$ and $b$ that will minimize this loss value across all of our training data samples.  We will use the common negative log-likelihood as the loss.  This is equivalent to maximizing the likelihood of the training set $D$ under the model parameterized by $W$ and $b$.  The loss is defined as:\n",
    "\n",
    "$$L=-\\sum_{i=0}^{|D|}log(P(Y=y^{(i)}|x^{(i)},W,B))$$\n",
    "\n",
    "In Theano, we define the negative log loss function in the following way by creating another method operating on our logistic regression class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def negative_log_likelihood(self, y):\n",
    "    # y corresponds to a vector that gives for each data sample\n",
    "    # the correct label\n",
    "    return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
    "\n",
    "LogisticRegression.negative_log_likelihood = negative_log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model we will use an algorithm called stochastic gradient descent (SGD) with mini-batches (sometimes also called minibatch gradient descent (MGD)).  We will not expose the complete details of SGD in this introductory class, but the essential algorithm proceeds as follows.  First, we feed-forward a mini-batch of data samples through the model to get probabilistic class predictions.  Second, we compute the loss by comparing the predictions to the true class labels we have as part of our dataset.  Third, we update the model parameters in such a way that next time we feed-forward the same mini-batch through the model the loss will be lower. \n",
    "\n",
    "In order to update the parameters we must compute whether to make each one bigger or smaller to reduce the loss.  We work this out by computing what are called the *gradients* of the loss function with respect to the parameters.  In mathematical terms these are the partial derivatives of the loss function with respect to each of the parameters.  Intuitively, these tell you which direction and with what magnitude you must modify each of the parameters to reduce the loss.\n",
    "\n",
    "Theano makes computing gradients very simple and this is a huge benefit.  Due to the symbolic definition of the loss function in terms of the parameters, we can simply call the Theano *grad* function to work out the gradients - no calculus required!\n",
    "\n",
    "When you execute the cell below you will define a Theano function called `train`.  Defining this function causes Theano to compile our symbolic definition of the SGD algorithm into efficient GPU-enabled code that will operate on our numerical input data.  Notice that at no point in defining this function does an explicit numerical value need to be assigned to any of our variables - it is more like writing math than code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize logistic regression model\n",
    "lr = LogisticRegression(shared_x, 2, 2)\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Allocate symbolic variables for the input data\n",
    "X = T.matrix('X')\n",
    "Y = T.ivector('Y')\n",
    "\n",
    "# Begin training function definition based on symbolic mathematical operations\n",
    "# Specify loss function\n",
    "cost = lr.negative_log_likelihood(Y)\n",
    "\n",
    "# Specify parameters to be updated by SGD algorithm\n",
    "params = lr.params\n",
    "\n",
    "# Compute gradients of parameters with respect to loss\n",
    "grads = T.grad(cost, params)\n",
    "\n",
    "# Compute parameter updates using SGD with momentum\n",
    "# We use the values 0.1 for learning rate and 0.9 for momentum\n",
    "updates = []  \n",
    "for p, g in zip(params, grads):\n",
    "    mparam_i = theano.shared(np.zeros(p.get_value().shape, dtype=theano.config.floatX))\n",
    "    v = 0.9 * mparam_i - learning_rate * g\n",
    "    updates.append((mparam_i, v))\n",
    "    updates.append((p, p + v))\n",
    "\n",
    "# Define theano function to be compiled to perform training update based on numerical data\n",
    "train = theano.function(\n",
    "    inputs=[],\n",
    "    outputs=cost,\n",
    "    updates=updates,\n",
    "    givens={\n",
    "        X: shared_x[:],\n",
    "        Y: shared_y[:]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you execute the cell below you will see our logistic regression model train on the randomly generated data. After every 10 training iterations you will see the decision boundary between the two classes updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize decision surface for display\n",
    "x_min, x_max = x[:, 0].min() - 0.1, x[:, 0].max() + 0.1\n",
    "y_min, y_max = x[:, 1].min() - 0.1, x[:, 1].max() + 0.1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, .01),\n",
    "                     np.arange(y_min, y_max, .01))\n",
    "\n",
    "# Perform 300 training iterations\n",
    "for i in range(0,300):\n",
    "    \n",
    "    # Perform SGD update\n",
    "    cost = train()\n",
    "    \n",
    "    # Compute decision surface\n",
    "    Z = np.argmax(lr.predict(np.c_[xx.ravel(), yy.ravel()].astype('float32')).eval(),1)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Every 10 iterations, visualize decision surface\n",
    "    if i%10==0:\n",
    "        display.clear_output(wait=True)\n",
    "        print 'Iteration ' + str(i) + ' of 300'\n",
    "        print 'Cost: ' + str(cost)\n",
    "        plt.contourf(xx, yy, Z, levels=[-1,0,1], colors=['b','r'])\n",
    "        plt.scatter(x[:,0], x[:,1], c=y, s=100)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We see that we are able to quickly learn a decision boundary between the two classes of points which allows the logistic regression function to accurately classify them.  Now let's see what happens if we have a more complicated distribution of points in our two classes.\n",
    "\n",
    "####Q #1: \n",
    "\n",
    "What is the effect of reducing the learning rate value in the train function?  Try setting the *learning_rate* variable to 0.001 and then executing the last two cells again.\n",
    "\n",
    "**A**: See [Answer #1 below](#Answer-#1)\n",
    "\n",
    "Next we will generate a new set of points where one class lies within the circle defined by $x^2+y^2=0.5$ and the other class lies outside it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate 100 random points\n",
    "# Class one is points inside x^2 + y^2 = 0.5\n",
    "# Class two is points outside x^2 + y^2 = 0.5\n",
    "x = 2 * np.random.random((100,2)).astype('float32') - 1\n",
    "y = np.zeros((100,)).astype('int32')\n",
    "y[np.sum(np.square(x),1) > 0.5] = 1\n",
    "\n",
    "# Plot the points \n",
    "plt.scatter(x[:,0], x[:,1], c=y, s=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you execute the cell below you will define a Theano function called `train`.  This function causes Theano to compile our symbolic definition of the SGD algorithm into efficient GPU-enabled code that will operate on our numerical input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define theano shared variables for the second set of input data and labels\n",
    "shared_x = theano.shared(np.asarray(x, dtype=theano.config.floatX), borrow=True)\n",
    "shared_y = theano.shared(np.asarray(y, dtype=theano.config.floatX), borrow=True)\n",
    "shared_y = T.cast(shared_y, 'int32')\n",
    "\n",
    "# Initialize logistic regression model\n",
    "lr = LogisticRegression(shared_x, 2, 2)\n",
    "\n",
    "# Allocate symbolic variables for the input data\n",
    "X = T.matrix('X')\n",
    "Y = T.ivector('Y')\n",
    "\n",
    "# Specify parameters to be updated by SGD algorithm\n",
    "params = lr.params\n",
    "\n",
    "# Begin training function definition based on symbolic mathematical operations\n",
    "# Specify loss function\n",
    "cost = lr.negative_log_likelihood(Y)\n",
    "\n",
    "# Compute gradients of parameters with respect to loss\n",
    "grads = T.grad(cost, params)\n",
    "\n",
    "# Compute parameter updates using SGD with momentum\n",
    "# We use the values 0.1 for learning rate and 0.9 for momentum\n",
    "updates = []    \n",
    "for p, g in zip(params, grads):\n",
    "    mparam_i = theano.shared(np.zeros(p.get_value().shape, dtype=theano.config.floatX))\n",
    "    v = 0.9 * mparam_i - 0.1 * g\n",
    "    updates.append((mparam_i, v))\n",
    "    updates.append((p, p + v))\n",
    "\n",
    "# Define theano function to be compiled to perform training update based on numerical data\n",
    "train = theano.function(\n",
    "    inputs=[],\n",
    "    outputs=cost,\n",
    "    updates=updates,\n",
    "    givens={\n",
    "        X: shared_x[:],\n",
    "        Y: shared_y[:]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you execute the cell below you will see our logistic regression model train on the second, more complicated, set of randomly generated data.  After every 10 training iterations you will see the decision boundary between the two classes updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize decision surface for display\n",
    "x_min, x_max = x[:, 0].min() - 0.1, x[:, 0].max() + 0.1\n",
    "y_min, y_max = x[:, 1].min() - 0.1, x[:, 1].max() + 0.1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, .01),\n",
    "                     np.arange(y_min, y_max, .01))\n",
    "\n",
    "# Perform 300 training iterations\n",
    "for i in range(0,300):\n",
    "    \n",
    "    # Perform SGD update\n",
    "    cost = train()\n",
    "    \n",
    "    # Compute decision surface\n",
    "    Z = np.argmax(lr.predict(np.c_[xx.ravel(), yy.ravel()].astype('float32')).eval(),1)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Every 10 iterations, visualize decision surface\n",
    "    if i%10==0:\n",
    "        display.clear_output(wait=True)\n",
    "        print 'Iteration ' + str(i) + ' of 300'\n",
    "        print 'Cost: ' + str(cost)\n",
    "        plt.contourf(xx, yy, Z, levels=[-1,0,1], colors=['b','r'])\n",
    "        plt.scatter(x[:,0], x[:,1], c=y, s=100)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This time we see that our logistic regression model is unable to find a decision boundary that can accurately seperate the two classes.  Due to the random parameter initialization and random data generation the exact outcome will be different each time, but often you will find that the model converges on a decision boundary where all data points get classified in the same class.  \n",
    "\n",
    "The reason this logistic regression model cannot accurately seperate the two classes is because it is limited to only finding a linear function, i.e. a straight line, that can seperate them.  In order to better classify this data we must increase the complexity of function that our model can learn - Theano enables us to easily do this as a logical extension to our current model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Example 2: Multilayer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As mentioned earlier, the logistic regression model we defined is equivalent to an artificial neural network with a single layer of input neurons fully connected to a single layer of output neurons.  Another name for a network of this type is a *(single-layer) perceptron*.  We can extend this simple model to give it the ability to learn more complex non-linear decision boundaries by adding another layer to the neural network between the inputs and the outputs.  We call this additional layer a *hidden* layer.  When we do so we create what is called a *multilayer perceptron*.  The mathematical form of this additional layer is exactly the same as before.  The only difference is that we now use a hyperbolic tangent (tanh) activation function in the first hidden layer and keep the softmax activation in the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class hidden(object):\n",
    "    \n",
    "    def __init__(self, input, n_in, n_out):\n",
    "\n",
    "        # Initialize pseudo-random number generator\n",
    "        rng = np.random.RandomState(1234)\n",
    "        \n",
    "        # Randomly generate weight matrix W\n",
    "        W_values = np.asarray(\n",
    "            rng.uniform(\n",
    "                low=-np.sqrt(6. / (n_in + n_out)),\n",
    "                high=np.sqrt(6. / (n_in + n_out)),\n",
    "                size=(n_in, n_out)\n",
    "            ),\n",
    "            dtype=theano.config.floatX\n",
    "        )\n",
    "\n",
    "        self.W = theano.shared(value=W_values, name='W', borrow=True)        \n",
    "        \n",
    "        # Randomly initialize bias vector b\n",
    "        b_values = np.zeros((n_out,), dtype=theano.config.floatX)\n",
    "        self.b = theano.shared(value=b_values, name='b', borrow=True)\n",
    "        \n",
    "        # Define feed-forward function\n",
    "        self.hidden = T.tanh(T.dot(input, self.W) + self.b)\n",
    "        \n",
    "        # Specify parameters of layer\n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "    def feedforward(self, input):\n",
    "        \n",
    "        # Function to apply layer feedforward to new input data\n",
    "        return T.tanh(T.dot(input, self.W) + self.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you execute the cell below you will define a Theano function called `train`.  This function causes Theano to compile our symbolic definition of the SGD algorithm into efficient GPU-enabled code that will operate on our numerical input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize multi-layer perceptron\n",
    "# i.e., initialize a hidden layer followed by a logistic regression layer\n",
    "layer0 = hidden(shared_x, 2, 10)\n",
    "layer1 = LogisticRegression(layer0.hidden, 10, 2)\n",
    "\n",
    "# Allocate symbolic variables for the input data\n",
    "X = T.matrix('X')\n",
    "Y = T.ivector('Y')\n",
    "\n",
    "# Begin training function definition based on symbolic mathematical operations\n",
    "# Specify loss function\n",
    "cost = layer1.negative_log_likelihood(Y)\n",
    "\n",
    "# Specify parameters to be updated by SGD algorithm\n",
    "params = layer0.params + layer1.params\n",
    "\n",
    "# Compute gradients of parameters with respect to loss\n",
    "grads = T.grad(cost, params)\n",
    "\n",
    "# Compute parameter updates using SGD with momentum\n",
    "# We use the values 0.1 for learning rate and 0.9 for momentum\n",
    "updates = []    \n",
    "for p, g in zip(params, grads):\n",
    "    mparam_i = theano.shared(np.zeros(p.get_value().shape, dtype=theano.config.floatX))\n",
    "    v = 0.9 * mparam_i - 0.1 * g\n",
    "    updates.append((mparam_i, v))\n",
    "    updates.append((p, p + v))\n",
    "\n",
    "# Define theano function to be compiled to perform training update based on numerical data\n",
    "train = theano.function(\n",
    "    inputs=[],\n",
    "    outputs=cost,\n",
    "    updates=updates,\n",
    "    givens={\n",
    "        X: shared_x[:],\n",
    "        Y: shared_y[:]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you execute the cell below you will see our multi-layer perceptron model train on the second, more complicated, set of randomly generated data.  After every 10 training iterations you will see the decision boundary between the two classes updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize decision surface for display\n",
    "x_min, x_max = x[:, 0].min() - 0.1, x[:, 0].max() + 0.1\n",
    "y_min, y_max = x[:, 1].min() - 0.1, x[:, 1].max() + 0.1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, .01),\n",
    "                     np.arange(y_min, y_max, .01))\n",
    "\n",
    "# Perform 300 training iterations\n",
    "for i in range(0,300):\n",
    "    \n",
    "    # Perform SGD update\n",
    "    cost = train()\n",
    "    \n",
    "    # Compute decision surface\n",
    "    Z = np.argmax(layer1.predict(layer0.feedforward((np.c_[xx.ravel(), yy.ravel()].astype('float32')))).eval(),1)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Every 10 iterations, visualize decision surface\n",
    "    if i%10==0:\n",
    "        display.clear_output(wait=True)\n",
    "        print 'Iteration ' + str(i) + ' of 300'\n",
    "        print 'Cost: ' + str(cost)\n",
    "        plt.contourf(xx, yy, Z, levels=[-1,0,1], colors=['b','r'])\n",
    "        plt.scatter(x[:,0], x[:,1], c=y, s=100)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now see that the addition of a second layer in our neural network allows us to learn the more complicated decision boundary required to seperate the two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "We have seen so far that in Theano we can define an artificial neural network layer-by-layer as a sequence of Python classes.  We have also seen that we can define the loss function as a method on the final layer's class. We can then define a Theano function to train this network and we can make use of the Theano grad function so that we don't have to analytically work out the gradients of our loss function.\n",
    "\n",
    "Furthermore, we have seen that Theano's close syntactical resemblance to NumPy makes it very easy to integrate Theano functions into a Python data processing pipeline.\n",
    "\n",
    "This basic recipe for defining and training artificial neural networks in Theano can be extended to develop the most complicated, cutting edge networks being used in Deep Learning research today.  To see some examples of more complicated network types being defined and trained in Theano I recommend you look at the [Deep Learning Tutorials](http://deeplearning.net/tutorial/) made available by the Theano developers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Example 3: Convolutional layers\n",
    "\n",
    "The `theano.tensor` sub-module offers many functions for operating on symbolic variables.  One such function is called `theano.tensor.signal.conv2d` and it is used for implementing the convolutional layers that are critical in computer vision applications.\n",
    "\n",
    "`theano.tensor.signal.conv2d` takes two symbolic inputs:\n",
    "\n",
    "- a 4D tensor corresponding to a mini-batch of input images.  The shape of the tensor is: [mini-batch size, number of input feature maps, image height, image width].\n",
    "- a 4D tensor corresponding to the weight matrix $W$.  The shape of this tensor is: [number of feature maps at layer m, number of feature maps at layer m-1, filter height, filter width].\n",
    "\n",
    "Below is Theano code for implementing a convolutional layer.  The layer has 3 input feature maps, i.e. an RGB image, of size 512x512.  We use two convolutional filters with 9x9 receptive fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from theano.tensor.nnet import conv\n",
    "\n",
    "# Initialize 4D tensor for input\n",
    "input = T.tensor4(name='input')\n",
    "\n",
    "# Specify height and width of convolutional filters\n",
    "filter_shape = (9, 9)\n",
    "\n",
    "# Initialize shared variable for weights\n",
    "w_shp = (2,3,filter_shape[0],filter_shape[1])\n",
    "w_bound = np.sqrt(3 * np.prod(filter_shape))\n",
    "W = theano.shared(np.asarray(\n",
    "        np.random.uniform(\n",
    "            low=-1.0 / w_bound,\n",
    "            high=1.0 / w_bound,\n",
    "            size=w_shp),\n",
    "        dtype=input.dtype), name='W')\n",
    "\n",
    "# build symbolic expression that computes the convolution\n",
    "# of the input with the filters in W\n",
    "conv_out = conv.conv2d(input, W)\n",
    "\n",
    "# build symbolic expression to apply sigmoid activation function\n",
    "# NOTE: for simplicity we are ommiting the bias vector that\n",
    "# would also normally be added\n",
    "output = T.nnet.sigmoid(conv_out)\n",
    "\n",
    "# create theano function to compute filtered images\n",
    "f = theano.function([input], output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens when we apply our layer to Lena..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import the lena image from skimage\n",
    "import skimage.data\n",
    "lena = skimage.data.lena()\n",
    "\n",
    "# put the image in 4D tensor of shape (1,3, height, width)\n",
    "img = np.asarray(lena, dtype='float32') / 256\n",
    "img_ = img.transpose(2, 0, 1).reshape(1, 3, 512, 512)\n",
    "\n",
    "# apply convolutional layer\n",
    "filtered_img = f(img_)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "# plot the original and first and second components of output\n",
    "plt.subplot(1, 3, 1); plt.axis('off'); plt.imshow(img)\n",
    "plt.gray();\n",
    "# recall that the convOp output (filtered image) is actually a \"minibatch\",\n",
    "# of size 1 here, so we take index 0 in the first dimension:\n",
    "plt.subplot(1, 3, 2); plt.axis('off'); plt.imshow(filtered_img[0, 0, :, :])\n",
    "plt.subplot(1, 3, 3); plt.axis('off'); plt.imshow(filtered_img[0, 1, :, :])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that randomly initialized filters act very much like edge detectors.\n",
    "\n",
    "####Q #2: \n",
    "\n",
    "What is the effect of changing the convolutional filter size?  Try the values (50, 50) and (2, 2) by modifying the ``filter_shape`` variable and then re-execute the last two cells.\n",
    "\n",
    "**A**: See [Answer #2 below](#Answer-#2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theano based Deep Learning libraries\n",
    "\n",
    "A number of lightweight Deep Learning focused libraries have been built using Theano.  Two of the most popular ones are [Lasagne](https://github.com/Lasagne/Lasagne) and [Keras](https://github.com/fchollet/keras).\n",
    "\n",
    "The motivation for developing these libraries is to create lightweight interfaces for Theano that are more concise for neural network specific development.  In other words, these libraries are less expressive than Theano in general but can enable more rapid development for Deep Learning specific use cases.\n",
    "\n",
    "Below is a demonstration of how to implement our multi-layer perceptron again, but this time using Keras.  Our choice to show Keras is arbitrary, we do not necessarily recommend it over Lasagne or any other similar library - we recommend you learn about each of them to find the best fit for your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import required keras sub-modules\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# Convert training labels to \"one-hot\" representation\n",
    "Y = np_utils.to_categorical(y, 2)\n",
    "\n",
    "# specify mlp model\n",
    "model = Sequential()\n",
    "model.add(Dense(2, 10, init='lecun_uniform'))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dense(10, 2, init='lecun_uniform'))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# specify training parameters\n",
    "sgd = SGD(lr=0.1, momentum=0.9)\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd)\n",
    "\n",
    "# train the model\n",
    "model.fit(x, Y, batch_size=100, nb_epoch=300, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we can define and train the same network in far fewer lines of code.\n",
    "\n",
    "Below we use the trained Keras model to generate our decision surface.  The actual code to use the model to generate new predictions is just one line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize decision surface for display\n",
    "x_min, x_max = x[:, 0].min() - 0.1, x[:, 0].max() + 0.1\n",
    "y_min, y_max = x[:, 1].min() - 0.1, x[:, 1].max() + 0.1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, .01),\n",
    "                     np.arange(y_min, y_max, .01))\n",
    "Z = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "# use keras model to make predictions across decision surface\n",
    "Z = model.predict(Z, batch_size=100,verbose=1)\n",
    "\n",
    "# Plot decision surface\n",
    "Z = np.argmax(Z, 1)\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.jet()\n",
    "plt.contourf(xx, yy, Z)\n",
    "plt.scatter(x[:,0], x[:,1], c=y, s=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Q #3: \n",
    "\n",
    "What is the effect of changing the batch size to 1 for training?  Do this by modifying the ``batch_size`` parameter in ``model.fit(...)`` above.  \n",
    "\n",
    "**A**: See [Answer #3 below](#Answer-#3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Lab Summary\n",
    "\n",
    "If you would like to download this lab for later viewing, it is recommended you go to your browsers File menu (not the Jupyter notebook file menu) and save the complete web page.  This will ensure the images are copied down as well.\n",
    "\n",
    "### More information\n",
    "\n",
    "For more information on using Theano, visit [http://deeplearning.net/software/theano/](http://deeplearning.net/software/theano/). A description of the framework, how to use it, and plenty of examples similar to this lesson are posted. \n",
    "\n",
    "To learn more about these other topics, please visit:\n",
    "* GPU accelerated machine learning: [http://www.nvidia.com/object/machine-learning.html](http://www.nvidia.com/object/machine-learning.html)\n",
    "* Theano: [http://deeplearning.net/software/theano/](http://deeplearning.net/software/theano/)\n",
    "* Torch: [http://torch.ch/](http://torch.ch/)\n",
    "* DIGITS: [https://developer.nvidia.com/digits](https://developer.nvidia.com/digits)\n",
    "* cuDNN: [https://developer.nvidia.com/cudnn](https://developer.nvidia.com/cudnn)\n",
    "\n",
    "### Deep Learning Lab Series\n",
    "\n",
    "Make sure to check out the rest of the classes in this Deep Learning lab series.  You can find them [here](https://developer.nvidia.com/deep-learning-courses).\n",
    "\n",
    "### Acknowledgements\n",
    "\n",
    "The logistic regression example used was inspired by Andrej Karpathy's [ConvnetJS](http://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html) demonstration.  The \"Key Theano concepts\" section was largely adapted from Colin Raffel's [Theano tutorial](http://nbviewer.ipython.org/github/craffel/theano-tutorial/blob/master/Theano%20Tutorial.ipynb).  The \"Convolutional layers\" section was adapted from the Theano Deep Learning tutorial [here](http://deeplearning.net/tutorial/lenet.html#lenet)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Lab Answers\n",
    "\n",
    "### Answer #1\n",
    "Reducing the learning rate causes the decision boundary to change much more slowly.\n",
    "\n",
    "[Return to question](#Q-#1:)\n",
    "\n",
    "### Answer #2\n",
    "You will see that larger filters caused a more blurred feature map and smaller filters caused a sharper feature map.\n",
    "\n",
    "[Return to question](#Q-#2:)\n",
    "\n",
    "### Answer #3\n",
    "Training becomes much slower.  This is because we are no longer able to benefit from the speed-up gained by parallelizing the processing of multiple input data samples at the same time on the GPU.\n",
    "\n",
    "[Return to question](#Q-#3:)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
